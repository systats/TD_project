---
title: "Meta-Analysis"
author: "Ben Guinaudeau & Simon Roth"
date: "*last updated:* `r Sys.Date()`"
output: html_notebook
---

# Abstract 
This is a brief explanation of Ben and my work. We try to collect all relevant data from google API. The first goal is to get all literture dependening on others by citation. Maybe the actual article text is also important for text clustering or similarity measures. The main benefit is a text-analysis based approach for literure reviewing. 

# Key Questions

# Code

## Google Scholar Micro Scraper

```{r}
rm(list=ls())
library(rvest)
library(ggplot2)

url_ajps <- "https://scholar.google.de/scholar?hl=en&as_publication=American+Journal+of+Political+Science&as_sdt=0%2C5&as_ylo=2000&as_yhi=2017"

# "https://scholar.google.de/scholar?start=10&hl=en&as_publication=American+Journal+of+Political+Science&as_sdt=0,5&as_ylo=2000&as_yhi=2017"


library(stringr)

str_scrape_journal <- function(url){
  
  # read file
  page_ajps <- xml2::read_html(url)
  
  # extract amount citations
  citations <- page_ajps %>% 
    html_nodes (".gs_fl > a:nth-child(1)") %>% 
    html_text() %>%
    str_extract("[[:digit:]]+") %>%
    as.numeric
    # str_extract("\\d\\d\\d\\d\\d|\\d\\d\\d\\d|\\d\\d\\d|\\d\\d|\\d")   
  
  # extract titles
  titles <- page_ajps %>% 
    html_nodes (".gs_rt") %>% 
    html_text()
  
  # ectract links
  links <- page_ajps %>% 
    html_nodes (".gs_fl > a:nth-child(1)") %>%
    html_attr("href")
  
  # ectract authors
  authors <- page_ajps %>% 
    html_nodes (".gs_a") %>%   
    html_text() %>% 
    str_replace("- .*?$", "")
    
  # extract journals
  journals <- page_ajps %>% 
    html_nodes (".gs_a") %>%   
    html_text() %>% 
    str_extract("- .*?,") %>%
    str_replace_all("-|,", "") %>% 
    str_trim %>% tolower
  
  # extract years
  years <- page_ajps %>% 
    html_nodes (".gs_a") %>%   
    html_text() %>%
    str_extract("\\d{3,4}") %>%
    as.numeric
  
  # gather data
  dt <- data.frame(titles, authors, years, citations, links, journals)
  
  return(dt)
}

library(dplyr)
dt <- str_scrape_journal(url_ajps)
glimpse(dt)
dt$journals
dt$authors
```


# Cheating Functions

## Time Delay
```{r}
# devtools::install_github("hadley/httr")
# install.packages(c("curl","httr"))
library(httr)

time_delay <- function(mean_delay = 5, sd_delay = 1){
  ### set random time
  delay <- abs(rnorm(1, mean_delay, sd_delay)) 
  # honstly this is probably unnecessary,
  # but it adds a little variation to life.
  
  ### set expectation
  cat("+++ Waiting for ", delay, " seconds +++\n")
  ### Now wait just a sec...
  Sys.sleep(delay)
}
```


## Change IP address
```{r}
rotate_ip <- function(){
  new_proxy <- "https://www.us-proxy.org/"
  # read file
  
  new_proxy <- read_html(new_proxy)
    
  proxyIP <- new_proxy %>% 
    html_nodes ("td:nth-child(1)") %>% 
    html_text()
  
  proxyPort <- new_proxy %>% 
    html_nodes(" td:nth-child(2)") %>% 
    html_text() %>% as.numeric()

  #confirm that proxy is live.
  # system2("ping", proxyIP)
  
  proxyName <-
  paste("http://", proxyIP[1], ":", proxyPort[1], sep = "")
  
  Sys.setenv(http_proxy = proxyName) #set environemental variable
}
```


## Scrape online political dict and fire on google

```{r}
pol_dict_search <- function(){
  #n_search <- sample(2:10, 1)
  search_letter <- sample(letters, 1)
  search_url <-
  paste("http://politicaldictionary.com/words/category/",
    search_letter, "/", sep = "")
  
  ### scrape political words
  pol_words <- xml2::read_html(search_url)
  pol_words <- pol_words %>%
    html_nodes(".entry-title a") %>%
    html_text() %>%
    str_replace_all(" ", "+")
  
  for(jj in 1:length(pol_words)){
      main <- "https://www.google.de/?gws_rd=ssl#q="
      # n_terms <- sample(2:5, 1)
      # rterm <- sample(tm::stopwords(kind = "eng"), n_terms, replace = F) %>%
      # paste(collapse = "+")
      cat(pol_words[jj], "\n")
      final <- paste(main, pol_words[jj], sep = "")
      xml2::read_html(final)
  }
}

pol_dict_search()
```


## Dictionary (data-base) search queries
```{r}
### 1.) random stop words based
random_search1 <- function(){
  n_search <- sample(2:10, 1)
  for(jj in 1:n_search){
    main <- "https://www.google.de/?gws_rd=ssl#q="
    n_terms <- sample(1:2, 1)
    rterm <- sample(tm::stopwords(kind = "eng"), n_terms, replace = F) %>%
      paste(collapse = "+")
    cat(rterm, "\n")
    final <- paste(main, rterm, sep = "")
    xml2::read_html(final)
  }
}

random_search1()


### 2.) random dictionary based
# https://github.com/dwyl/english-words
random_search2 <- function(){
  my_dict <- data.table::fread("dict_eng/words_alpha.txt")
  n_search <- sample(2:7, 1)
    for(jj in 1:n_search){
    main <- "https://www.google.de/?gws_rd=ssl#q="
    n_terms <- sample(1:2, 1)
    rterm <- sample(my_dict$a, n_terms, replace = F) %>%
      paste(collapse = "+")
    cat(rterm, "\n")
    final <- paste(main, rterm, sep = "")
    xml2::read_html(final)
  }
}

random_search2()


### 3.) random pronoun + random noun 
random_search3 <- function(){
  my_dict <- data.table::fread("dict_eng/words_alpha.txt")
  n_search <- sample(2:15, 1)
    for(jj in 1:n_search){
    main <- "https://www.google.de/?gws_rd=ssl#q="
    # n_terms <- sample(1:2, 1)
    rdict <- sample(my_dict$a, 1, replace = F) %>%
      paste(collapse = "+")
    rpronoun <- sample(tm::stopwords(kind = "eng"), 1, replace = F)
    rterm <- paste(rpronoun, rdict, sep = " ") %>%
      str_replace_all(" ", "+")
    
    cat(rterm, "\n")
    final <- paste(main, rterm, sep = "")
    xml2::read_html(final)
  }
}

random_search3()
```

## Change user agent identification

```{r}
# # Approach #2 (Less Polite)
#  - Identify Yourself Differently -

#  This approach can also be a handy way to retrieve mobile versions of web pages.
#  For a non-definative list of user agents from many different device types, check out:
#  https://deviceatlas.com/blog/list-of-user-agent-strings

# Current user agent can be accessed with getOption("HTTPUserAgent"). It is also listed in str(options())

windows10 <- "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 Edge/12.246"
chrome_book <- "Mozilla/5.0 (X11; CrOS x86_64 8172.45.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.64 Safari/537.36"
mac <- "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9"
windows7 <- "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.111 Safari/537.36"
google_pix <- "Mozilla/5.0 (Linux; Android 7.0; Pixel C Build/NRD90M; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/52.0.2743.98 Safari/537.36"
sony_xz4 <- "Mozilla/5.0 (Linux; Android 6.0.1; SGP771 Build/32.2.A.0.253; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/52.0.2743.98 Safari/537.36"
nvidia_shield <- "Mozilla/5.0 (Linux; Android 5.1.1; SHIELD Tablet Build/LMY48C) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.98 Safari/537.36"
sasmung_tab_a <- "Mozilla/5.0 (Linux; Android 5.0.2; SAMSUNG SM-T550 Build/LRX22G) AppleWebKit/537.36 (KHTML, like Gecko) SamsungBrowser/3.3 Chrome/38.0.2125.102 Safari/537.36"
amazon_kindle_f7 <- "Mozilla/5.0 (Linux; Android 4.4.3; KFTHWI Build/KTU84M) AppleWebKit/537.36 (KHTML, like Gecko) Silk/47.1.79 like Chrome/47.0.2526.80 Safari/537.36"
lg_g7 <- "Mozilla/5.0 (Linux; Android 5.0.2; LG-V410/V41020c Build/LRX22G) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/34.0.1847.118 Safari/537.36"

user_agents <- c(windows10,
                 chrome_book,
                 mac,
                 windows7, 
                 google_pix, 
                 sony_xz4, 
                 nvidia_shield, 
                 sasmung_tab_a, 
                 amazon_kindle_f7, 
                 lg_g7
                 )

change_user_agent <- function(){
  # newUrl <- "http://displaymyip.com/"
  new_agent <- sample(user_agents, size = 1, replace = T)
  options("HTTPUserAgent" = new_agent)
}

### Test function 
# Ben: while loop ;)

#ua <- 1
while(ua < 1000) {
  change_user_agent()
  print(options("HTTPUserAgent"))
  ua <- ua + 1
  Sys.sleep(1)
}
```


## Other ideas

* Randomize search queries. Search for unimportant topics sequentially. 
* Increase elements per site 20 instead of 10
* bigger time intervals to get most of the search results
* change 00-20-40...
* max: 420-450 search hits (# 68%) oder 44 iterations with random_search3.


### Packages 

* [RSelenium](https://cran.r-project.org/web/packages/RSelenium/vignettes/RSelenium-saucelabs.html)
* [`httr` vignette](https://cran.r-project.org/web/packages/httr/vignettes/quickstart.html)
* There is also a package called [`iptools`](https://cran.r-project.org/web/packages/iptools/iptools.pdf)




### Alternativ citation source

* [crossref](https://support.crossref.org/hc/en-us/articles/213534146-Cited-by-overview)
* https://developers.google.com/apis-explorer/?csw=1#p/



### Journal Ranking from 

[scimagojr](http://www.scimagojr.com/journalrank.php?category=3312)

**saved as xlsx, but need as csv**

```{r}
library(haven)
# install.packages("xlsx")
jrankings <- read.csv("journal_ranking_2016.csv", header = T, sep = ";")
glimpse(jrankings)
```

# Scrape Google

```{r}
closeAllConnections()
set.seed(123)
library(httr)

s <- seq(00, 1300, by = 20)
index_page <- sprintf("%02d", s)
index_page1 <- sample(index_page, size = length(index_page), replace = F)

#cbind(index_page, index_page1)

links_page <- paste("https://scholar.google.de/scholar?start=", index_page, "&hl=en&as_publication=American+Journal+of+Political+Science&as_sdt=0,5&as_ylo=2000&as_yhi=2017", sep = "")

articles <- list()
pb <- txtProgressBar(min = 0, max = length(index_page), style = 3)


for(jj in seq_along(links_page)){
    tryCatch({ 
    
    ### scrape articles (10 per round)
    articles[[jj]] <- str_scrape_journal(links_page[jj])
    articles[[jj]][, "step"] <- jj

  }, error = function(e) {
    message("no file found!")
    message(e)
    cat('\n')
  
  }, finally = {
    ### finally change process bar
    setTxtProgressBar(pb, jj)
    cat("\n")
    
    ### polity scraling
    rotate_ip()
    change_user_agent()
    
    ### random choose random generator
    which.func <- sample(1:3, 1)
    if(which.func == 1){
      random_search1()
    } else if(which.func == 2){
      random_search2()
    } else {
      random_search3()
    }

    time_delay(mean_delay = 3, sd_delay = 1)
  })
}

close(pb)

# rebind list
library(data.table)
articles1 <- do.call(rbind, articles)
articles2 <- articles1 %>% dplyr::arrange(citations)
articles2


# new
# new_url <- paste("https://scholar.google.de", articles1$links, sep = "")
# dt <- str_scrape_journal(new_url[1])
# new_url

# save(articles1, file = "sample_data.Rdata")
```


# Explore Results
## average amount of new citations

```{r, echo = T, results = "asis", message = F, warnings = F}
load("sample_data.Rdata")

articles1 <- articles1 %>%
  mutate(citations = as.numeric(citations)) %>%
  mutate(cit_log = log(as.numeric(citations)))
  

glimpse(articles1)
library(ggplot2)
articles1 %>%
  ggplot(aes(years, citations)) +
    geom_jitter() +
    geom_smooth() + ylim(0,2000)

articles1 %>%
  select(years, citations) %>% 
  #mutate(cit_log = log(citations)) %>% 
  tidyr::gather(variable, value) %>%
  ggplot(aes(value)) +
    geom_histogram() + 
    facet_wrap(~variable, scales = "free")
```

```{r, echo = F, results = "asis", message = F, warnings = F}
articles1 <- articles1 %>% 
  mutate(years_rev = 2017 - years)
glimpse(articles1)

fit_cit <- lm(citations ~ years_rev, data = articles1)
summary(fit_cit)

library(stargazer, quietly = TRUE)
stargazer(fit_cit, type = "html")

# library(texreg)
# htmlreg(fit_cit)

library(sjPlot)
sjp.lm(fit_cit)$plot + geom_smooth(color = "red", alpha = .1, se = F)
```


put time penality on

```{r}
articles1 %>%
  # mutate(index_time = 2017 - dates) %>%
  mutate(cit_cor = citations - (years_rev*fit_cit$coefficients[2])) %>%
  ggplot(aes(years_rev, cit_cor)) +
    geom_jitter(width = .2) +
    geom_smooth()
    annotate(geom = "rect", xmin = 0, xmax = 17, ymin = -200, ymax = 400, alpha = .3, fill = "red")

```





