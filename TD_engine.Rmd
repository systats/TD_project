---
title: "Meta-Analysis"
output: html_notebook
---

# Abstract 
This is a brief explanation of Ben and my work. We try to collect all relevant data from google API. The first goal is to get all literture dependening on others by citation. Maybe the actual article text is also important for text clustering or similarity measures. The main benefit is a text-analysis based approach for literure reviewing. 

# Key Questions

# Code


```{r}
rm(list=ls())
library(rvest)
library(ggplot2)

url_ajps <- "https://scholar.google.de/scholar?as_q=&as_epq=&as_oq=&as_eq=&as_occt=any&as_sauthors=&num=20&as_publication=American+Journal+of+Political+Science&as_ylo=2000&as_yhi=2017&btnG=&hl=en&as_sdt=0%2C5"

# "https://scholar.google.de/scholar?start=10&hl=en&as_publication=American+Journal+of+Political+Science&as_sdt=0,5&as_ylo=2000&as_yhi=2017"


library(stringr)

str_scrape_journal <- function(url){
  
  # read file
  page_ajps <- xml2::read_html(url)
  
  # extract amount citations
  citations <- page_ajps %>% 
    html_nodes (".gs_fl > a:nth-child(1)") %>% 
    html_text() %>%
    str_extract("[[:digit:]]+") %>%
    as.numeric
    # str_extract("\\d\\d\\d\\d\\d|\\d\\d\\d\\d|\\d\\d\\d|\\d\\d|\\d")   
  
  # extract titles
  titles <- page_ajps %>% 
    html_nodes (".gs_rt") %>% 
    html_text()
  
  # ectract links
  links <- page_ajps %>% 
    html_nodes (".gs_fl > a:nth-child(1)") %>%
    html_attr("href")
  
  # ectract authors
  authors <- page_ajps %>% 
    html_nodes (".gs_a") %>%   
    html_text() %>% 
    str_replace("- .*?$", "")
    
  # extract journals
  journals <- page_ajps %>% 
    html_nodes (".gs_a") %>%   
    html_text() %>% 
    str_extract("- .*?,") %>%
    str_replace_all("-|,", "") %>% 
    str_trim %>% tolower
  
  # extract years
  years <- page_ajps %>% 
    html_nodes (".gs_a") %>%   
    html_text() %>%
    str_extract("\\d{3,4}") %>%
    as.numeric
  
  # gather data
  dt <- data.frame(titles, authors, years, citations, links, journals)
  
  return(dt)
}

library(dplyr)
dt <- str_scrape_journal(url_ajps)
glimpse(dt)
dt$journals
dt$authors
```



[`httr` vignette](https://cran.r-project.org/web/packages/httr/vignettes/quickstart.html)

There is also a package called [`iptools`](https://cran.r-project.org/web/packages/iptools/iptools.pdf)


```{r}
# devtools::install_github("hadley/httr")
# install.packages(c("curl","httr"))
library(httr)

time_delay <- function(mean_delay = 5, sd_delay = 1){
  ### set random time
  delay <- abs(rnorm(1, mean_delay, sd_delay)) 
  # honstly this is probably unnecessary,
  # but it adds a little variation to life.
  
  ### set expectation
  cat("\n Waiting for ", delay, " seconds.")
  ### Now wait just a sec...
  Sys.sleep(delay)
}

rotate_ip <- function(){
  new_proxy <- "https://www.us-proxy.org/"
  # read file
  
  new_proxy <- read_html(new_proxy)
    
  proxyIP <- new_proxy %>% 
    html_nodes ("td:nth-child(1)") %>% 
    html_text()
  
  proxyPort <- new_proxy %>% 
    html_nodes(" td:nth-child(2)") %>% 
    html_text() %>% as.numeric()

  #confirm that proxy is live.
  # system2("ping", proxyIP)
  
  proxyName <-
  paste("http://", proxyIP[1], ":", proxyPort[1], sep = "")
  
  Sys.setenv(http_proxy = proxyName) #set environemental variable
}

```

```{r}
# # Approach #2 (Less Polite)
#  - Identify Yourself Differently -

#  This approach can also be a handy way to retrieve mobile versions of web pages.
#  For a non-definative list of user agents from many different device types, check out:
#  https://deviceatlas.com/blog/list-of-user-agent-strings

# Current user agent can be accessed with getOption("HTTPUserAgent"). It is also listed in str(options())

windows10 <- "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 Edge/12.246"
chrome_book <- "Mozilla/5.0 (X11; CrOS x86_64 8172.45.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.64 Safari/537.36"
mac <- "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9"
windows7 <- "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.111 Safari/537.36"

user_agents <- c(windows10, chrome_book, mac, windows7)
### more user agents?

change_user_agent <- function(){
  # newUrl <- "http://displaymyip.com/"
  new_agent <- sample(user_agents, size = 1, replace = T)
  options(HTTPUserAgent = new_agent)
}


change_user_agent()
# time out was reached

options("HTTPUserAgent")
# options(HTTPUserAgent = "My settings")
```


[RSelenium](https://cran.r-project.org/web/packages/RSelenium/vignettes/RSelenium-saucelabs.html)

```{r}
#Load dependencies
# devtools::install_github("ropensci/RSelenium", force=T)
library(RSelenium)

#Access Chrome driver
# RSelenium::rsDriver()
startServer(javaargs = "") #path to where chromedriver is located on local hard (downloaded from: https://sites.google.com/a/chromium.org/chromedriver/downloads)
remDr <- remoteDriver(browserName = "chrome") 
remDr$open()

#Navigate to url, read, and sparse html table into dataframe
remDr$navigate("http://priceonomics.com/hotels/rankings/#airbnb-apartments-all")
doc <- htmlParse(remDr$getPageSource()[[1]])
doc<-readHTMLTable(doc)
data2<-doc[[2]]
```



* Randomize search queries. Search for unimportant topics sequentially. 
* Increase elements per site 20 instead of 10
* Small time intervals to get most of the search results
* change 00-40-80...


```{r}
set.seed(123)
library(httr)

s <- seq(00, 1300, by = 10)
index_page <- sprintf("%02d", s)
index_page1 <- sample(index_page, size = length(index_page), replace = F)

#cbind(index_page, index_page1)

links_page <- paste("https://scholar.google.de/scholar?start=", index_page, "&hl=en&as_publication=American+Journal+of+Political+Science&as_sdt=0,5&as_ylo=2000&as_yhi=2017", sep = "")

articles <- list()
pb <- txtProgressBar(min = 0, max = length(index_page), style = 3)


for(jj in seq_along(links_page)){
    tryCatch({ 
    
    ### scrape articles (10 per round)
    articles[[jj]] <- str_scrape_journal(links_page[jj])
    
    ### polity scraling
    time_delay(mean_delay = 2, sd_delay = 1)
    rotate_ip()
    change_user_agent()
    
  }, error = function(e) {
    message("no file found!")
    message(e)
    cat('\n')
  
  }, finally = {
    ### finally change process bar
    setTxtProgressBar(pb, jj)
  })
}

close(pb)

# rebind list
library(data.table)
articles1 <- do.call(rbind, articles)
articles2 <- articles1 %>% dplyr::arrange(citations)
articles2

rep(index_page1, each = 10)

# new
# new_url <- paste("https://scholar.google.de", articles1$links, sep = "")
# dt <- str_scrape_journal(new_url[1])
# new_url

# save(articles1, file = "sample_data.Rdata")
```



## average amount of new citations

```{r, echo = T, results = "asis", message = F, warnings = F}
load("sample_data.Rdata")

articles1 <- articles1 %>%
  mutate(citations = as.numeric(citations)) %>%
  mutate(cit_log = log(as.numeric(citations)))
  

library(ggplot2)
articles1 %>%
  ggplot(aes(dates, citations)) +
    geom_jitter() +
    geom_smooth()

articles1 %>%
  select(dates, citations) %>% 
  #mutate(cit_log = log(citations)) %>% 
  tidyr::gather(variable, value) %>%
  ggplot(aes(value)) +
    geom_histogram() + 
    facet_wrap(~variable, scales = "free")
```

```{r, echo = F, results = "asis", message = F, warnings = F}
fit_cit <- lm(citations ~ dates, data = articles1)
summary(fit_cit)

library(stargazer, quietly = TRUE)
stargazer(fit_cit, type = "html")

# library(texreg)
# htmlreg(fit_cit)

library(sjPlot)
sjp.lm(fit_cit)$plot + geom_smooth(color = "red", alpha = .1, se = F)
```


put time penality on

```{r}
articles1 %>%
  mutate(index_time = 2017 - dates) %>%
  mutate(cit_cor = citations - (index_time*fit_cit$coefficients[2])) %>%
  ggplot(aes(dates, cit_cor)) +
    geom_jitter(width = .2) +
    geom_smooth() + 
    annotate(geom = "rect", xmin = 1999, xmax = 2016, ymin = 150, ymax = 400, alpha = .3, fill = "red")

```





